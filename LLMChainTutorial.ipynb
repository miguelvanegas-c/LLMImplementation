{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18007c7e",
   "metadata": {},
   "source": [
    "## Installing Dependencies\n",
    "\n",
    "The required libraries are installed to work with LangChain and connect to local models through Ollama. Without these dependencies, the rest of the notebook cannot run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4bef5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in .\\.venv\\Lib\\site-packages (1.2.10)\n",
      "Requirement already satisfied: langchain-community in .\\.venv\\Lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: langchain_Ollama in .\\.venv\\Lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.10 in .\\.venv\\Lib\\site-packages (from langchain) (1.2.14)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.8 in .\\.venv\\Lib\\site-packages (from langchain) (1.0.9)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in .\\.venv\\Lib\\site-packages (from langchain) (2.12.5)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in .\\.venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in .\\.venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (0.7.6)\n",
      "Requirement already satisfied: packaging>=23.2.0 in .\\.venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (26.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in .\\.venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in .\\.venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (9.1.4)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in .\\.venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in .\\.venv\\Lib\\site-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (0.14.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in .\\.venv\\Lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.10->langchain) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in .\\.venv\\Lib\\site-packages (from langgraph<1.1.0,>=1.0.8->langchain) (4.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.8 in .\\.venv\\Lib\\site-packages (from langgraph<1.1.0,>=1.0.8->langchain) (1.0.8)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in .\\.venv\\Lib\\site-packages (from langgraph<1.1.0,>=1.0.8->langchain) (0.3.8)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in .\\.venv\\Lib\\site-packages (from langgraph<1.1.0,>=1.0.8->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in .\\.venv\\Lib\\site-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.8->langchain) (1.12.2)\n",
      "Requirement already satisfied: httpx>=0.25.2 in .\\.venv\\Lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.11.5 in .\\.venv\\Lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain) (3.11.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in .\\.venv\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in .\\.venv\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in .\\.venv\\Lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in .\\.venv\\Lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain) (4.12.1)\n",
      "Requirement already satisfied: certifi in .\\.venv\\Lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in .\\.venv\\Lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in .\\.venv\\Lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in .\\.venv\\Lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in .\\.venv\\Lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in .\\.venv\\Lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in .\\.venv\\Lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in .\\.venv\\Lib\\site-packages (from langchain-community) (1.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in .\\.venv\\Lib\\site-packages (from langchain-community) (2.0.46)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in .\\.venv\\Lib\\site-packages (from langchain-community) (3.13.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in .\\.venv\\Lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in .\\.venv\\Lib\\site-packages (from langchain-community) (2.13.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in .\\.venv\\Lib\\site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=2.1.0 in .\\.venv\\Lib\\site-packages (from langchain-community) (2.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in .\\.venv\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in .\\.venv\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in .\\.venv\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in .\\.venv\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in .\\.venv\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in .\\.venv\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in .\\.venv\\Lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in .\\.venv\\Lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in .\\.venv\\Lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in .\\.venv\\Lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in .\\.venv\\Lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in .\\.venv\\Lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.venv\\Lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain) (2.6.3)\n",
      "Requirement already satisfied: greenlet>=1 in .\\.venv\\Lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in .\\.venv\\Lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.6.0 in .\\.venv\\Lib\\site-packages (from langchain_Ollama) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain langchain-community langchain_Ollama \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a31df",
   "metadata": {},
   "source": [
    "## Basic Agent with a Custom Tool\n",
    "\n",
    "A minimal agent is built to demonstrate the full flow: loading a model, registering a tool, and getting a response. This serves as a starting point before adding complexity to the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a1ef10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='a0178d29-b1e8-4034-890c-9bb99db970f4'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen2.5:7b', 'created_at': '2026-02-22T04:33:11.1193794Z', 'done': True, 'done_reason': 'stop', 'total_duration': 14126779300, 'load_duration': 174079400, 'prompt_eval_count': 141, 'prompt_eval_duration': 10583668800, 'eval_count': 20, 'eval_duration': 3344267400, 'logprobs': None, 'model_name': 'qwen2.5:7b', 'model_provider': 'ollama'}, id='lc_run--019c839f-3c9f-7cd2-aeac-9b79692b43c5-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'f64c1217-58d8-4381-aa8c-6b392568bb13', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 141, 'output_tokens': 20, 'total_tokens': 161}), ToolMessage(content=\"It's always sunny in sf!\", name='get_weather', id='5158ec7d-d027-4e0e-95ed-a11984e876b2', tool_call_id='f64c1217-58d8-4381-aa8c-6b392568bb13'), AIMessage(content='The weather in San Francisco is always sunny!', additional_kwargs={}, response_metadata={'model': 'qwen2.5:7b', 'created_at': '2026-02-22T04:33:16.5487904Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5421644100, 'load_duration': 263051700, 'prompt_eval_count': 184, 'prompt_eval_duration': 3212020800, 'eval_count': 10, 'eval_duration': 1929123100, 'logprobs': None, 'model_name': 'qwen2.5:7b', 'model_provider': 'ollama'}, id='lc_run--019c839f-73d6-7263-b35b-0b199447e6a6-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 184, 'output_tokens': 10, 'total_tokens': 194})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"qwen2.5:7b\",      # debe coincidir con `ollama list`\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "out = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf1d53",
   "metadata": {},
   "source": [
    "## Defining the System Prompt\n",
    "\n",
    "The agent's personality, role, and behavioral instructions are established here. The system prompt determines how the agent reasons and when it should use each available tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4229043",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n",
    "\n",
    "You have access to two tools:\n",
    "\n",
    "- get_weather_for_location: use this to get the weather for a specific location\n",
    "- get_user_location: use this to get the user's location\n",
    "\n",
    "If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd81979",
   "metadata": {},
   "source": [
    "## Tools with User Context\n",
    "\n",
    "The tools the agent can invoke are defined here, including a context schema that allows passing user information (such as their ID) securely at runtime, without exposing it directly in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7182c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "@tool\n",
    "def get_weather_for_location(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "@dataclass\n",
    "class Context:\n",
    "    \"\"\"Custom runtime context schema.\"\"\"\n",
    "    user_id: str\n",
    "\n",
    "@tool\n",
    "def get_user_location(runtime: ToolRuntime[Context]) -> str:\n",
    "    \"\"\"Retrieve user information based on user ID.\"\"\"\n",
    "    user_id = runtime.context.user_id\n",
    "    return \"Florida\" if user_id == \"1\" else \"SF\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d3c8a",
   "metadata": {},
   "source": [
    "## Language Model Configuration\n",
    "\n",
    "The local model (Qwen 2.5 via Ollama) is configured here. This is the model the agent will use to reason, decide which tools to call, and generate final responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40ec8dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"qwen2.5:7b\",   # debe coincidir con `ollama list`\n",
    "    temperature=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f3f77a",
   "metadata": {},
   "source": [
    "## Structured Response Schema\n",
    "\n",
    "The output format that the agent must follow when responding is defined here. This ensures the response is predictable and typed, making it easier to use in applications that need to process the agent's data reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f44cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "# We use a dataclass here, but Pydantic models are also supported.\n",
    "@dataclass\n",
    "class ResponseFormat:\n",
    "    \"\"\"Response schema for the agent.\"\"\"\n",
    "    # A punny response (always required)\n",
    "    punny_response: str\n",
    "    # Any interesting information about the weather if available\n",
    "    weather_conditions: str | None = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7b316",
   "metadata": {},
   "source": [
    "## Conversation Memory\n",
    "\n",
    "In-memory storage is enabled so the agent can remember the history of previous messages within a conversation thread, allowing multi-turn interactions with persistent context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e55a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "checkpointer = InMemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff32848",
   "metadata": {},
   "source": [
    "## Assembling and Running the Full Agent\n",
    "\n",
    "All previous components (model, tools, context, response format, and memory) are brought together into a single agent and tested with a two-turn conversation to verify that reasoning, memory, and structured output work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9fe1a769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ha! Always sunny in Floridaâ€”except when itâ€™s not. But don't worry, I've got you covered with a sunny side up prediction for tomorrow. Enjoy your day under the... well, mostly clear skies!\n",
      "\n",
      "For more accurate weather conditions, feel free to ask again later or specify a different location if needed.\n",
      "You're welcome! If you need any more punny weather updates, just holler. Stay sunny, even when it's not! ðŸŒžðŸ˜‰\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    tools=[get_user_location, get_weather_for_location],\n",
    "    context_schema=Context,\n",
    "    response_format=ToolStrategy(ResponseFormat),\n",
    "    checkpointer=checkpointer\n",
    ")\n",
    "\n",
    "# `thread_id` is a unique identifier for a given conversation.\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather tomorrow?\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"1\")\n",
    ")\n",
    "\n",
    "structured = response.get(\"structured_response\")\n",
    "print(structured if structured is not None else response[\"messages\"][-1].content)\n",
    "# ResponseFormat(\n",
    "#     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n",
    "#     weather_conditions=\"It's always sunny in Florida!\"\n",
    "# )\n",
    "\n",
    "\n",
    "# Note that we can continue the conversation using the same `thread_id`.\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"1\")\n",
    ")\n",
    "structured = response.get(\"structured_response\")\n",
    "print(structured if structured is not None else response[\"messages\"][-1].content)\n",
    "# ResponseFormat(\n",
    "#     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n",
    "#     weather_conditions=None\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
